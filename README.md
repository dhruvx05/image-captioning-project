# ğŸ–¼ï¸ Image Captioning Project

This project implements an image captioning model that generates natural language captions for images. It uses deep learning components built with PyTorch, including a CNN image encoder and two types of sequence decoders: **LSTM** and **Transformer**.

---

## âœ… Components Used

### ğŸ§  Models

- **ResNet-50** (pretrained): Used as the image encoder to extract feature vectors from input images.
- **LSTM Decoder**: A sequence decoder that generates captions using LSTM cells with attention.
- **Transformer Decoder**: Custom-built Transformer decoder using multi-head attention and positional encoding.

---

### ğŸ§° Libraries Used

- **PyTorch / TorchVision** â€“ Deep learning and pretrained ResNet model
- **NLTK** â€“ Caption tokenization and vocabulary building
- **Datasets (Hugging Face)** â€“ Dataset handling (if used)
- **Pillow** â€“ Image processing
- **OpenCV** â€“ Image manipulation
- **NumPy** â€“ General numerical operations

> ğŸ”¸ `ultralytics` and YOLO are **not used** in this project.

---

## ğŸ“Š Evaluation Metrics

- **BLEU Score** â€“ Measures the similarity between generated captions and reference captions
- **Word Precision** â€“ Measures overlap between generated and true words

---

## ğŸ“ File

- `image_captioning_transformer_.ipynb` â€” Main Jupyter notebook containing all code, training, and evaluation logic.

---

